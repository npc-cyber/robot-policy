# transformer的

## 单头注意力中的qkv 
维度的变化 注意此处 dk == dv 
| tensor | shape |
|-|-|
| q  | [b dq n] |
| k  | [b dk n] |
| v  | [b dv n] |
| s  | q @ k.t = [b dq dk] |
| z  | s @ v = [b dq n]  |

## 多头注意力
q [b h dq n] 其实就是多了一个h 维度, 相当于h个单注意力层的结果cat在一起罢了, 只不过这样可以并行去做
h个层相当于h个专家系统

## 注意力中的 qkv是人们自己给的意义
| 向量 | 含义 | 类比 | 是否“记忆” |
|-|-|-|-|
| Query (Q) | “我想知道什么？” | 搜索关键词 | ❌ 不是记忆，是查询请求 |		
|Key (K)|“我如何被找到？”|数据库索引/标签|⚠️ 是记忆的“索引”|		
|Value (V)|“我实际包含什么？”|数据库中的实际内容|✅ 是真正的“记忆”或信息内容|		
